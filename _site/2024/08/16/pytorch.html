<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>PyTorch</title>
  <link rel="stylesheet" href="/assets/css/style.css">
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      display: flex;
      min-height: 100vh;
      overflow-x: hidden;
    }

    .sidebar {
      width: 200px;
      background-color: #f4f4f4;
      padding: 20px;
      position: fixed;
      height: 100vh;
      top: 0;
      left: 0;
    }

    .content-wrapper {
      margin-left: 220px; /* Ensure space for the sidebar */
      padding: 20px;
      width: calc(100% - 220px); /* Prevent overflow beyond viewport */
      box-sizing: border-box;
    }

    .content {
      max-width: 100%; /* Make sure content doesn't exceed available width */
      padding: 20px;
    }

    .sidebar a {
      display: block;
      padding: 10px 0;
      color: #333;
      text-decoration: none;
    }

    .sidebar a:hover {
      background-color: #ddd;
    }

    .category-list, .post-list {
      margin-top: 20px;
    }
  </style>
</head>
<body>

  <div class="sidebar">
    <h2><a href="">Appzoid</a></h2>
    <a href="/2024/09/23/learning-neural-networks">Learning Neural Nets</a>
    <a href="/2024/07/24/llm-models">Large Language Models</a>
    <a href="/2024/07/24/vision">Computer Vision</a>
    <a href="/2024/08/16/pytorch">PyTorch</a>
    <a href="/2024/08/16/rag">RAG</a>
    <a href="/2024/07/24/backpropagation">Backpropagation</a>
    <a href="/machine-learning-and-deep-learning/readme.html">ML and DL with PyTorch</a>
  </div>

  <div class="content-wrapper">
    <div class="content">
      <h1 id="broadcasting-semantics">Broadcasting semantics</h1>

<p>Q: Can I divide a 27x27 matrix by a 27x1 ?
A: Yes, it will be broadcasted.</p>

<p><a href="https://pytorch.org/docs/stable/notes/broadcasting.html">Broadcasting semantics</a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Two tensors are “broadcastable” if the following rules hold:

Each tensor has at least one dimension.

When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.
</code></pre></div></div>

<h1 id="pytorch-multinomial">Pytorch multinomial</h1>

<p>Q: How to sample from a multinomial distribution?
A: Use <a href="https://pytorch.org/docs/stable/generated/torch.multinomial.html">torch.multinomial</a></p>

<h1 id="likelihood">Likelihood</h1>

<p>We have a model with probabilities assigned for each possible bigram. And we have known words that are composed of these bigrams. We want the product of all these probabilities for bigrams found in words to be as high as possible. That’s the likelihood.</p>

<p>Now, because the probabilities are between 0 and 1, we can’t multiply them together directly as we will end up with a very small number. But we can add the logarithm of each probability. This is called the <a href="https://en.wikipedia.org/wiki/Log_likelihood">log-likelihood</a>.</p>

<p>log(a<em>b</em>c) = log(a)+log(b)+log(c)</p>

<p>The hgher the probabilities, the higher the likelihood. But because for a loss function lower is better, we want to maximize the likelihood and not minimize. Hence, we use negative of the log-likelihood as our loss function.</p>

<p>Some would prefer to use an average nll.</p>

<p>GOAL: minimize the average negative log likelihood.</p>

<p>For out training set, the average negative log likelihood is 2.4541.</p>

<h1 id="model-as-a-neural-network">Model as a neural network</h1>

<p>PyTorch has Tensor and tensor. We’ll use tensor.</p>

<h1 id="mlp---multi-layer-perceptron">MLP - Multi layer perceptron</h1>

<p>We have a vocabulary of 17000 words. We want to map each works into a 30 dimensional vector. That’s the size of the embedding matrix. The input is the word index, and the output is the embedding for that word. We keep the embeddings into a matrix called C.</p>

<h1 id="cross-entropy">Cross entropy</h1>

<p>The sequence: logits -&gt; exponentiate -&gt; probability -&gt; loss can be replaced with F.cross_entropy(logits, Y)</p>

<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy">PyTorch Cross entropy loss</a></p>

    </div>
  </div>

</body>
</html>
