<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LLM Models</title>
  <link rel="stylesheet" href="/assets/css/style.css">
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      display: flex;
      min-height: 100vh;
      overflow-x: hidden;
    }

    .sidebar {
      width: 200px;
      background-color: #f4f4f4;
      padding: 20px;
      position: fixed;
      height: 100vh;
      top: 0;
      left: 0;
    }

    .content-wrapper {
      margin-left: 220px; /* Ensure space for the sidebar */
      padding: 20px;
      width: calc(100% - 220px); /* Prevent overflow beyond viewport */
      box-sizing: border-box;
    }

    .content {
      max-width: 100%; /* Make sure content doesn't exceed available width */
      padding: 20px;
    }

    .sidebar a {
      display: block;
      padding: 10px 0;
      color: #333;
      text-decoration: none;
    }

    .sidebar a:hover {
      background-color: #ddd;
    }

    .category-list, .post-list {
      margin-top: 20px;
    }
  </style>
</head>
<body>

  <div class="sidebar">
    <h2><a href="">Appzoid</a></h2>
    <a href="/2024/09/23/learning-neural-networks">Learning Neural Nets</a>
    <a href="/2024/07/24/llm-models">Large Language Models</a>
    <a href="/2024/07/24/vision">Computer Vision</a>
    <a href="/2024/08/16/pytorch">PyTorch</a>
    <a href="/2024/08/16/rag">RAG</a>
    <a href="/2024/07/24/backpropagation">Backpropagation</a>
    <a href="/machine-learning-and-deep-learning/readme.html">ML and DL with PyTorch</a>
  </div>

  <div class="content-wrapper">
    <div class="content">
      <p><a href="https://www.youtube.com/watch?v=bZQun8Y4L2A">Source Video</a></p>

<h1 id="stages">Stages</h1>

<ul>
  <li>Pre-training -&gt; Base model</li>
  <li>Supervised training -&gt; SFT model (assistants, for example the ChatGPT is an assistent on top of the GPT4 base model)</li>
  <li>Reward modeling  -&gt; RM model (humans choosing from multiple completions for a prompt, ranking them and then the model is * retrained to match the human’s choice)</li>
  <li>Reinforcement learning -&gt; RL model (based on the rewards given for each completion, lower the tokens part of a non rewarded completion and increase the rewarded ones)</li>
</ul>

<p><a href="https://www.youtube.com/watch?v=l8pRSuU81PU">Source Video</a></p>

<h1 id="gpt-2">GPT-2</h1>

<p>A vocabulary of 50257 tokens.
Based on tiktoken.
Vercel app available to tokenize in the browser https://tiktokenizer.vercel.app/?model=gpt2</p>

<p>Based on UTF-8 encoding and byte-pair encoding algorithm:</p>
<ul>
  <li>start with a string to tokenize</li>
  <li>get the utf-8 byte array for the string (elements between 0-255)</li>
  <li>count the occurences for each consecutive byte-pair</li>
  <li>for the pair with the max occurence, replace it with the next available ID (256 for the first replacement)</li>
  <li>now the vocabulary size is +1 but the input is shorter</li>
  <li>reiterate until you reach a desired vocabulary size</li>
</ul>

<p>raw text &lt;-&gt; Unicode sequence &lt;-&gt; tokenizer &lt;-&gt; token sequence &lt;-&gt; LLM</p>

<h1 id="embedding-table">Embedding table</h1>

<p>Encoding of the vocabulary: 50257 rows, one for each token, with ? columns, the dimension of the embedding vector.
Embeddings are trainable parameters of the model, similar tokens end up close to each other in the embeddings space.</p>

<h1 id="attention-layer">Attention layer</h1>

<p>The context size is 1024 tokens so in the attention layer a token is connected to at most the previous 1023 tokens.</p>

<h1 id="training-speed-ups">Training speed ups</h1>

<p>A Mac M2 has 3.6 TFLOPs (trillion floating point operations per second) on 160 execution units or 1280 ALUs.
Note: floating point is used instead of integers in order to model probability distributions.</p>

<p>Autocast not yet available in torch for MPS https://github.com/pytorch/pytorch/pull/99272 so can’t use bflops16</p>

<p><a href="https://arxiv.org/pdf/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a>
<a href="https://arxiv.org/pdf/2307.08691">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a>
<a href="https://arxiv.org/pdf/1805.02867">Online normalizer calculation for softmax</a></p>

<h1 id="ddp---distributed-data-parallel">DDP - Distributed Data Parallel</h1>

<p>Distribute processing across multiple machines and GPUs.
Local rank: the rank of the GPU on a single machine, starting from 0.
Global rank: the rank of the GPU across all machines, starting from 0.</p>

<p>PyTorch provides DistributedDataParallel (DDP) which is a wrapper around torch.nn.parallel.DistributedDataParallel that handles the communication between processes.</p>

<p>A model is wrapped in DDP and then:</p>
<ul>
  <li>nothing changes on the forward pass</li>
  <li>the backward pass is modified to handle gradients from multiple GPUs. It will aggregate (average) the gradients across all GPUs.</li>
</ul>

<h1 id="datasets">Datasets</h1>

<ul>
  <li>Tiny Shakespeare</li>
  <li>WebText (Open WebText) - used  by OpenAI GPT-2 (~40GB); oudbound links from Reddit comments</li>
  <li>Common crawl - ~1TB of text (800M words)</li>
  <li>WebText2</li>
  <li>RedPajama, SlimPajama</li>
  <li>FineWeb, FineWebEdu</li>
  <li>Wikipedia</li>
</ul>

<h2 id="loading-finewebedu-dataset">Loading FineWebEdu dataset</h2>

<p><a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu">HiggingFace DataSet</a>
Using Python dataset package:</p>

<ol>
  <li>load the dataset with dataset.load_dataset(‘HuggingFaceFW/fineweb-edu’, split=’train’)</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Tokenize each document in the dataset. Each tokenized document starts with &lt;</td>
          <td>endoftexttoken</td>
          <td>&gt;</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Load the tokens into numpy array of type uint64.</li>
  <li>Output is saved to shards as numpy files, each with 100 million tokens.</li>
</ol>

<p>When loading the tokens:</p>
<ol>
  <li>Load the numpy file.</li>
  <li>Convert to torch.Tensor of type long.</li>
</ol>

<h1 id="validation">Validation</h1>

<p><a href="https://arxiv.org/abs/1905.07830">HellaSwag</a></p>

<p>A data set with sentences and 4 potential completions for each, where a single one is correct.</p>

<p>the sentence is feeded into the model and the probabilities for each of the 4 possible completions areevaluated. The highest probability should be for the correct answer. The individual token probabilities are averaged.</p>

    </div>
  </div>

</body>
</html>
